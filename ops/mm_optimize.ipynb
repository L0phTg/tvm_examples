{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed for deferring annotation parsing in TVMScript\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import relax as R\n",
    "from tvm.script import tir as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import te\n",
    "from tvm import topi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文章(TIR)：\n",
    "- https://mlc.ai/zh/chapter_tensor_program/case_study.html\n",
    "- https://mlc.ai/zh/chapter_end_to_end/index.html\n",
    "\n",
    "参考文章(TE)：\n",
    "- https://tvm.hyper.ai/docs/how_to/optimize/cpu_conv\n",
    "- https://tvm.hyper.ai/docs/how_to/te_schedules/primitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyModule:\n",
    "    @T.prim_func\n",
    "    def main(\n",
    "        A: T.Buffer[(1024, 1024), \"float32\"],\n",
    "        B: T.Buffer[(1024, 1024), \"float32\"],\n",
    "        C: T.Buffer[(1024, 1024), \"float32\"],\n",
    "    ):\n",
    "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
    "        for i, j, k in T.grid(1024, 1024, 1024):\n",
    "            with T.block(\"C\"):\n",
    "                vi, vj, vk = T.axis.remap(\"SSR\", [i, j, k])\n",
    "                with T.init():\n",
    "                    C[vi, vj] = 0.0\n",
    "                C[vi, vj] = C[vi, vj] + A[vi, vk] * B[vk, vj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = \"float32\"\n",
    "a_np = np.random.rand(1024, 1024).astype(dtype)\n",
    "b_np = np.random.rand(1024, 1024).astype(dtype)\n",
    "\n",
    "a_nd = tvm.nd.array(a_np)\n",
    "b_nd = tvm.nd.array(b_np)\n",
    "c_nd = tvm.nd.empty((1024, 1024), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy执行矩阵乘法的花费时间："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy running time: 0.020675\n"
     ]
    }
   ],
   "source": [
    "np_repeat = 20\n",
    "np_runing_time = timeit.timeit(\n",
    "    setup=\"import numpy\\n\"\n",
    "    \"M = \" + str(1024) + \"\\n\"\n",
    "    \"K = \" + str(1024) + \"\\n\"\n",
    "    \"N = \" + str(1024) + \"\\n\"\n",
    "    'dtype = \"float32\"\\n'\n",
    "    \"a = numpy.random.rand(M, K).astype(dtype)\\n\"\n",
    "    \"b = numpy.random.rand(K, N).astype(dtype)\\n\",\n",
    "    stmt=\"answer = numpy.dot(a, b)\",\n",
    "    number=np_repeat,\n",
    ")\n",
    "print(\"Numpy running time: %f\" % (np_runing_time / np_repeat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tir基线，未经过优化的性能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost of MyModule: 2.756632 s\n"
     ]
    }
   ],
   "source": [
    "lib = tvm.build(MyModule, target=\"llvm\")\n",
    "f_timer_before = lib.time_evaluator(\"main\", tvm.cpu())\n",
    "print(\"Time cost of MyModule: %f s\" % (f_timer_before(a_nd, b_nd, c_nd).mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始优化："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一：分块\n",
    "1. 对循环进行拆分：（按照L1 cache的大小，拆分出block）\n",
    "2. 对循环进行重排序，(block spatial axis放在最内部, reduce axis放在最外部)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1d 缓存：       32K\n",
      "L1i 缓存：       32K\n",
      "L2 缓存：        256K\n",
      "L3 缓存：        12288K\n"
     ]
    }
   ],
   "source": [
    "!lscpu | grep \"cache\" # 或者\"缓存\", 我这儿是32KB, 选择分配给Block 4 * 64 * 64 = 16KB 给Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_mm(sch: tvm.tir.Schedule, jfactor=4):\n",
    "    block_C = sch.get_block(\"C\", \"main\")\n",
    "    i, j, k = sch.get_loops(block=block_C)\n",
    "    j_0, j_1 = sch.split(loop=j, factors=[None, jfactor])\n",
    "    sch.reorder(i, j_0, k, j_1)\n",
    "    sch.decompose_reduction(block_C, k)\n",
    "    return sch\n",
    "\n",
    "def schedule_mm2(sch: tvm.tir.Schedule, bn=64, kfactor=4):\n",
    "    block_C = sch.get_block(\"C\", \"main\")\n",
    "    i, j, k = sch.get_loops(block=block_C)\n",
    "    # 对spatial进行循环拆分, bn*bn*sizeof(float)=4KB, L1 cache \n",
    "    i_0, i_1 = sch.split(loop=i, factors=[None, bn])\n",
    "    j_0, j_1 = sch.split(loop=j, factors=[None, bn])\n",
    "    # 对reduce进行循环拆分\n",
    "    k_0, k_1 = sch.split(loop=k, factors=[None, kfactor])\n",
    "    # 进行循环重排序, 将 reduction 域提升到 分块循环 之外\n",
    "    sch.reorder(i_0, j_0, k_0, k_1, i_1, j_1)\n",
    "    return sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = tvm.tir.Schedule(MyModule)\n",
    "sch = schedule_mm2(sch, bn=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #1E90FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">main</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">1024</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">1024</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], C: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">1024</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]):\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>, <span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i_0, j_0, k_0, k_1, i_1, j_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">256</span>, <span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
       "                vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1024</span>, i_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">32</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> i_1)\n",
       "                vj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1024</span>, j_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">32</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> j_1)\n",
       "                vk <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">1024</span>, k_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> k_1)\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[vi, vk], B[vk, vj])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(C[vi, vj])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    C[vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                C[vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> C[vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[vi, vk] <span style=\"color: #AA22FF; font-weight: bold\">*</span> B[vk, vj]\n",
       "    \n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sch.mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost of MyModule=>schedule_mm: 0.336546 s\n"
     ]
    }
   ],
   "source": [
    "lib = tvm.build(sch.mod, target=\"llvm\")\n",
    "f_timer_after = lib.time_evaluator(\"main\", tvm.cpu())\n",
    "print(\"Time cost of MyModule=>schedule_mm: %f s\" % (f_timer_after(a_nd, b_nd, c_nd).mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二、向量化\n",
    "\n",
    "另一个重要技巧是向量化，当内存访问模式一致时，编译器可以检测到这种模式并将连续内存传递给向量处理器。TVM 中可以用 `vectorize` 接口来提示编译器这种模式，这样就可以进行加速。\n",
    "\n",
    "本教程选择向量化内部循环 row data（对缓存更友好）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_mm3(sch: tvm.tir.Schedule, bn=64, kfactor=4):\n",
    "    block_C = sch.get_block(\"C\", \"main\")\n",
    "    i, j, k = sch.get_loops(block=block_C)\n",
    "    # 一、分块\n",
    "    # 对spatial进行循环拆分, bn*bn*sizeof(float)=4KB, L1 cache \n",
    "    i_0, i_1 = sch.split(loop=i, factors=[None, bn])\n",
    "    j_0, j_1 = sch.split(loop=j, factors=[None, bn])\n",
    "    # 对reduce进行循环拆分\n",
    "    k_0, k_1 = sch.split(loop=k, factors=[None, kfactor])\n",
    "    # 进行循环重排序, 将 reduction 域提升到 分块循环 之外\n",
    "    sch.reorder(i_0, j_0, k_0, k_1, i_1, j_1)\n",
    "\n",
    "    # 二、向量化：向量化内部循环(对缓存更友好)\n",
    "    sch.vectorize(j_1)\n",
    "    return sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = tvm.tir.Schedule(MyModule)\n",
    "sch = schedule_mm3(sch, bn=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost of MyModule=>schedule_mm: 0.322161 s\n"
     ]
    }
   ],
   "source": [
    "lib = tvm.build(sch.mod, target=\"llvm\")\n",
    "f_timer_after = lib.time_evaluator(\"main\", tvm.cpu())\n",
    "print(\"Time cost of MyModule=>schedule_mm: %f s\" % (f_timer_after(a_nd, b_nd, c_nd).mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #1E90FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">main</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">1024</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">1024</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], C: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">1024</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]):\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>, <span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i_0, j_0, k_0, k_1, i_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">256</span>, <span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">64</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">for</span> j_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">64</span>):\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
       "                    vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1024</span>, i_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> i_1)\n",
       "                    vj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1024</span>, j_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> j_1)\n",
       "                    vk <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">1024</span>, k_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> k_1)\n",
       "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[vi, vk], B[vk, vj])\n",
       "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(C[vi, vj])\n",
       "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                        C[vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                    C[vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> C[vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[vi, vk] <span style=\"color: #AA22FF; font-weight: bold\">*</span> B[vk, vj]\n",
       "    \n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sch.mod.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三、循环置换\n",
    "\n",
    "查看上面的 IR，可以看到内部循环的 row data 对于 B 和 C 都是向量化的（影响到了vi, vj, 进而对B的读和C的写实现了向量化）。\n",
    "\n",
    "接下来查看 A 的访问模式。在当前调度中，A 是逐列访问的，但它对缓存不友好。（因为抛去向量化的j_1后，i_1是最内部的循环，i_1的更新导致了vi的更新，进而导致了A的逐列访问）\n",
    "\n",
    "**如果改变 k_1 和内轴 i_1 的嵌套循环顺序，A 矩阵的访问模式对缓存更友好**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_mm4(sch: tvm.tir.Schedule, bn=64, kfactor=4):\n",
    "    block_C = sch.get_block(\"C\", \"main\")\n",
    "    i, j, k = sch.get_loops(block=block_C)\n",
    "    # 一、分块\n",
    "    # 对spatial进行循环拆分, bn*bn*sizeof(float)=4KB, L1 cache \n",
    "    i_0, i_1 = sch.split(loop=i, factors=[None, bn])\n",
    "    j_0, j_1 = sch.split(loop=j, factors=[None, bn])\n",
    "    # 对reduce进行循环拆分\n",
    "    k_0, k_1 = sch.split(loop=k, factors=[None, kfactor])\n",
    "    # 进行循环重排序, 将 reduction 域提升到 分块循环 之外\n",
    "    sch.reorder(i_0, j_0, k_0, k_1, i_1, j_1)\n",
    "\n",
    "    # 二、向量化：向量化内部循环(对缓存更友好)\n",
    "    sch.vectorize(j_1)\n",
    "\n",
    "    # 三、循环置换，改变A矩阵的访问模式, 将逐列访问访问逐行访问\n",
    "    sch.reorder(i_0, j_0, k_0, i_1, k_1, j_1)\n",
    "    return sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = tvm.tir.Schedule(MyModule)\n",
    "sch = schedule_mm4(sch, bn=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost of MyModule=>schedule_mm: 0.212235 s\n"
     ]
    }
   ],
   "source": [
    "lib = tvm.build(sch.mod, target=\"llvm\")\n",
    "f_timer_after = lib.time_evaluator(\"main\", tvm.cpu())\n",
    "print(\"Time cost of MyModule=>schedule_mm: %f s\" % (f_timer_after(a_nd, b_nd, c_nd).mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #1E90FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">main</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">1024</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">1024</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], C: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">1024</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]):\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>, <span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i_0, j_0, k_0, i_1, k_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">256</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">4</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">for</span> j_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">64</span>):\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
       "                    vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1024</span>, i_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> i_1)\n",
       "                    vj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1024</span>, j_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> j_1)\n",
       "                    vk <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">1024</span>, k_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> k_1)\n",
       "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[vi, vk], B[vk, vj])\n",
       "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(C[vi, vj])\n",
       "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                        C[vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                    C[vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> C[vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[vi, vk] <span style=\"color: #AA22FF; font-weight: bold\">*</span> B[vk, vj]\n",
       "    \n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sch.mod.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "四、数组打包\n",
    "\n",
    "> 解决B的访问模式\n",
    "\n",
    "另一个重要的技巧是数组打包，对多维数组的存储进行重新排序，展平并存储在一维内存中，方便顺序访问。\n",
    "\n",
    "当前B的访问模式：在做BLOCK内部的矩阵乘法时（[64, 4\\*64]@[64\\*4, 64]）：\n",
    "- 此时A的第二个维度4*64在一行中；\n",
    "- 而此时B的第一个维度的64*4不在一个维度中，而在一个[4, 64]的block中；\n",
    "\n",
    "![](../pics/array-packing.png)\n",
    "\n",
    "观察展平后 B 的数组访问模式，当迭代 K 维时，它不是顺序的。\n",
    "\n",
    "可以用维度 [K][N] 对 B 重新排序，使其具有 [N/bn][K][bn] 维度，其中 bn 是分块因子，也是内循环中 B 的向量大小。\n",
    "\n",
    "这种重新排序将 N 拆分为两个维度——bigN（N/bn）和 littleN（bn）——新维度 [N/bn][K][bn] 匹配 B 从外部到内部循环的索引（no, ko, ki, ni) 在展平后导致 B 的顺序访问模式。\n",
    "\n",
    "【注意】：\n",
    "- 这里实现的关键在于对[N/bn]的并行访问，才提高了效率；?????感觉这里做的不如直接转置效果好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def te_matmul(A: te.Tensor, B: te.Tensor, bn=64) -> te.Tensor:\n",
    "    assert A.shape[1] == B.shape[0]\n",
    "    m = A.shape[0]\n",
    "    n = B.shape[1]\n",
    "    K = A.shape[1]\n",
    "    packedB = te.compute((tvm.tir.indexdiv(n, bn), K, bn), lambda bigN, k, littleN: B[k, bigN * bn + littleN], name=\"packedB\")\n",
    "    k = te.reduce_axis((0, K), name=\"k\")\n",
    "    # 提前将j的block轴拆分出来\n",
    "    return te.compute((m, tvm.tir.indexdiv(n, bn), bn), \n",
    "                lambda i, j_0, j_1: te.sum(\n",
    "                    A[i, k] * packedB[j_0, k, j_1], axis=k), name=\"C\")\n",
    "\n",
    "# 注意，此时需要重新编写mm的算法\n",
    "def schedule_mm5(sch: tvm.tir.Schedule, bn=64, kfactor=4):\n",
    "    block_C = sch.get_block(\"C\", \"main\")\n",
    "    i, j_0, j_1, k = sch.get_loops(block=block_C)\n",
    "    # 一、分块\n",
    "    # 对spatial进行循环拆分, bn*bn*sizeof(float)=4KB, L1 cache \n",
    "    i_0, i_1 = sch.split(loop=i, factors=[None, bn])\n",
    "    # j_0, j_1 = sch.split(loop=j, factors=[None, bn]), 因为前面matmul已经拆分过了, 所以不再拆分j\n",
    "    # 对reduce进行循环拆分\n",
    "    k_0, k_1 = sch.split(loop=k, factors=[None, kfactor])\n",
    "    # 进行循环重排序， 将 reduction 域提升到 分块循环 之外\n",
    "    sch.reorder(i_0, j_0, k_0, k_1, i_1, j_1)\n",
    "\n",
    "    # 二、向量化：向量化内部循环(对缓存更友好)\n",
    "    sch.vectorize(j_1)\n",
    "\n",
    "    # 三、循环置换：改变A矩阵的访问模式, 将逐列访问访问逐行访问\n",
    "    sch.reorder(i_0, j_0, k_0, i_1, k_1, j_1)\n",
    "\n",
    "    # 四、数组打包：改变B矩阵的访问模式，下面的sch只是对packB做优化, \n",
    "    block_B = sch.get_block(\"packedB\", \"main\")\n",
    "    b_i, _, b_k = sch.get_loops(block=block_B)\n",
    "    sch.parallel(b_i)\n",
    "    sch.vectorize(b_k)\n",
    "\n",
    "    return sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = te.placeholder((1024, 1024), name=\"A\", dtype=\"float32\")\n",
    "B = te.placeholder((1024, 1024), name=\"B\", dtype=\"float32\")\n",
    "C = te_matmul(A, B, bn=64)\n",
    "te_matmul_func = te.create_prim_func([A, B, C])\n",
    "MyModule2 = tvm.IRModule({\"main\": te_matmul_func})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch2 = tvm.tir.Schedule(MyModule2)\n",
    "sch2 = schedule_mm5(sch2, bn=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost of MyModule=>schedule_mm: 0.168757 s\n"
     ]
    }
   ],
   "source": [
    "# 这里的C\n",
    "c_nd = tvm.nd.empty((1024, 1024/64, 64), dtype=\"float32\")\n",
    "\n",
    "lib = tvm.build(sch2.mod, target=\"llvm\")\n",
    "f_timer_after = lib.time_evaluator(\"main\", tvm.cpu())\n",
    "print(\"Time cost of MyModule=>schedule_mm: %f s\" % (f_timer_after(a_nd, b_nd, \n",
    "        c_nd).mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #1E90FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">main</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">1024</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">1024</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], C: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]):\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        packedB <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">64</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>parallel(<span style=\"color: #008000\">16</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">for</span> i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>serial(<span style=\"color: #008000\">1024</span>):\n",
       "                <span style=\"color: #008000; font-weight: bold\">for</span> i2 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">64</span>):\n",
       "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;packedB&quot;</span>):\n",
       "                        bigN, k, littleN <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSS&quot;</span>, [i0, i1, i2])\n",
       "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(B[k, bigN <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> littleN])\n",
       "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(packedB[bigN, k, littleN])\n",
       "                        packedB[bigN, k, littleN] <span style=\"color: #AA22FF; font-weight: bold\">=</span> B[k, bigN <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> littleN]\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0_0, i1, i3_0, i0_1, i3_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">256</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">4</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">for</span> i2 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">64</span>):\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
       "                    i <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1024</span>, i0_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> i0_1)\n",
       "                    j_0, j_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i1, i2])\n",
       "                    k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">1024</span>, i3_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> i3_1)\n",
       "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[i, k], packedB[j_0, k, j_1])\n",
       "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(C[i, j_0, j_1])\n",
       "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                        C[i, j_0, j_1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                    C[i, j_0, j_1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> C[i, j_0, j_1] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[i, k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> packedB[j_0, k, j_1]\n",
       "    \n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sch2.mod.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "五、块的写缓存\n",
    "\n",
    "分块后，程序会逐块将结果写入 C（访问模式不是顺序的）\n",
    "\n",
    "因此，可以使用顺序缓存数组来保存块结果，并在所有块结果准备好时写入 C。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_mm6(sch: tvm.tir.Schedule, bn=64, kfactor=4):\n",
    "    block_C = sch.get_block(\"C\", \"main\")\n",
    "    # 六、分配写缓存块\n",
    "    C_global = sch.cache_write(block_C, 0, \"global\")\n",
    "\n",
    "    i, j_0, j_1, k = sch.get_loops(block=block_C)\n",
    "    \n",
    "    # 一、分块\n",
    "    # 对spatial进行循环拆分, bn*bn*sizeof(float)=4KB, L1 cache \n",
    "    i_0, i_1 = sch.split(loop=i, factors=[None, bn])\n",
    "    k_0, k_1 = sch.split(loop=k, factors=[None, kfactor])\n",
    "    # 进行循环重排序， 将 reduction 域提升到 分块循环 之外\n",
    "    sch.reorder(i_0, j_0, k_0, k_1, i_1, j_1)\n",
    "    \n",
    "    # 六、循环展开：增加并发执行的机会\n",
    "    sch.unroll(k_1)\n",
    "\n",
    "    # 二、向量化：向量化内部循环(对缓存更友好)\n",
    "    sch.vectorize(j_1)\n",
    "\n",
    "    # 三、循环置换：改变A矩阵的访问模式, 将逐列访问访问逐行访问\n",
    "    sch.reorder(i_0, j_0, k_0, i_1, k_1, j_1)\n",
    "\n",
    "    # 四、数组打包：改变B矩阵的访问模式，下面的sch只是对packB做优化, \n",
    "    block_B = sch.get_block(\"packedB\", \"main\")\n",
    "    b_i, _, b_k = sch.get_loops(block=block_B)\n",
    "    sch.parallel(b_i)\n",
    "    sch.vectorize(b_k)\n",
    "    \n",
    "    # 六、写缓存块：改变C矩阵的写入 \n",
    "    sch.reverse_compute_at(C_global, j_0)\n",
    "    _, _, c_0, c_1 = sch.get_loops(block=C_global)\n",
    "    sch.vectorize(c_1)\n",
    "\n",
    "    return sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch2 = tvm.tir.Schedule(MyModule2)\n",
    "sch2 = schedule_mm6(sch2, bn=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost of MyModule=>schedule_mm: 0.139549 s\n"
     ]
    }
   ],
   "source": [
    "c_nd = tvm.nd.empty((1024, 1024/64, 64), dtype=\"float32\")\n",
    "\n",
    "lib = tvm.build(sch2.mod, target=\"llvm\")\n",
    "f_timer_after = lib.time_evaluator(\"main\", tvm.cpu())\n",
    "print(\"Time cost of MyModule=>schedule_mm: %f s\" % (f_timer_after(a_nd, b_nd, \n",
    "        c_nd).mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "六、并行化\n",
    "\n",
    "此外，还可以利用多核处理器进行线程级并行化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_mm7(sch: tvm.tir.Schedule, bn=64, kfactor=4):\n",
    "    block_C = sch.get_block(\"C\", \"main\")\n",
    "    # 六、分配写缓存块\n",
    "    C_global = sch.cache_write(block_C, 0, \"global\")\n",
    "\n",
    "    i, j_0, j_1, k = sch.get_loops(block=block_C)\n",
    "    \n",
    "    # 一、分块\n",
    "    # 对spatial进行循环拆分, bn*bn*sizeof(float)=4KB, L1 cache \n",
    "    i_0, i_1 = sch.split(loop=i, factors=[None, bn])\n",
    "    k_0, k_1 = sch.split(loop=k, factors=[None, kfactor])\n",
    "    # 进行循环重排序， 将 reduction 域提升到 分块循环 之外\n",
    "    sch.reorder(i_0, j_0, k_0, k_1, i_1, j_1)\n",
    "    \n",
    "    # 六、循环展开：增加并发执行的机会\n",
    "    sch.unroll(k_1)\n",
    "\n",
    "    # 二、向量化：向量化内部循环(对缓存更友好)\n",
    "    sch.vectorize(j_1)\n",
    "\n",
    "    # 三、循环置换：改变A矩阵的访问模式, 将逐列访问访问逐行访问\n",
    "    sch.reorder(i_0, j_0, k_0, i_1, k_1, j_1)\n",
    "\n",
    "    # 四、数组打包：改变B矩阵的访问模式，下面的sch只是对packB做优化, \n",
    "    block_B = sch.get_block(\"packedB\", \"main\")\n",
    "    b_i, _, b_k = sch.get_loops(block=block_B)\n",
    "    sch.parallel(b_i)\n",
    "    sch.vectorize(b_k)\n",
    "    \n",
    "    # 六、写缓存块：改变C矩阵的写入 \n",
    "    sch.reverse_compute_at(C_global, j_0)\n",
    "    _, _, c_0, c_1 = sch.get_loops(block=C_global)\n",
    "    sch.vectorize(c_1)\n",
    "\n",
    "    # 七、写缓存块：加入并行\n",
    "    sch.parallel(c_0)\n",
    "\n",
    "    return sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch2 = tvm.tir.Schedule(MyModule2)\n",
    "sch2 = schedule_mm7(sch2, bn=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost of MyModule=>schedule_mm: 0.150843 s\n"
     ]
    }
   ],
   "source": [
    "c_nd = tvm.nd.empty((1024, 1024/64, 64), dtype=\"float32\")\n",
    "\n",
    "lib = tvm.build(sch2.mod, target=\"llvm\")\n",
    "f_timer_after = lib.time_evaluator(\"main\", tvm.cpu())\n",
    "print(\"Time cost of MyModule=>schedule_mm: %f s\" % (f_timer_after(a_nd, b_nd, \n",
    "        c_nd).mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
