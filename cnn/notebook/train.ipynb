{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import fx\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed for deferring annotation parsing in TVMScript\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import relax as R\n",
    "from tvm.script import tir as T\n",
    "\n",
    "from tvm import te\n",
    "from tvm import topi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                aten::conv2d         0.70%     180.000us        57.81%      14.848ms      14.848ms             1  \n",
      "           aten::convolution         1.28%     329.000us        57.11%      14.668ms      14.668ms             1  \n",
      "          aten::_convolution         0.17%      44.000us        55.83%      14.339ms      14.339ms             1  \n",
      "    aten::mkldnn_convolution        55.52%      14.260ms        55.66%      14.295ms      14.295ms             1  \n",
      "                 aten::empty         0.11%      27.000us         0.11%      27.000us      27.000us             1  \n",
      "           aten::as_strided_         0.03%       8.000us         0.03%       8.000us       8.000us             1  \n",
      "               aten::sigmoid        15.83%       4.065ms        15.83%       4.065ms       4.065ms             1  \n",
      "            aten::avg_pool2d         8.56%       2.199ms         8.56%       2.199ms       2.199ms             1  \n",
      "                aten::conv2d         0.03%       8.000us         9.27%       2.380ms       2.380ms             1  \n",
      "           aten::convolution         0.11%      27.000us         9.24%       2.372ms       2.372ms             1  \n",
      "          aten::_convolution         0.06%      16.000us         9.13%       2.345ms       2.345ms             1  \n",
      "    aten::mkldnn_convolution         9.00%       2.311ms         9.07%       2.329ms       2.329ms             1  \n",
      "                 aten::empty         0.06%      16.000us         0.06%      16.000us      16.000us             1  \n",
      "           aten::as_strided_         0.01%       2.000us         0.01%       2.000us       2.000us             1  \n",
      "               aten::sigmoid         2.10%     540.000us         2.10%     540.000us     540.000us             1  \n",
      "            aten::avg_pool2d         2.44%     627.000us         2.44%     627.000us     627.000us             1  \n",
      "               aten::flatten         0.22%      56.000us         0.28%      72.000us      72.000us             1  \n",
      "        aten::_reshape_alias         0.06%      16.000us         0.06%      16.000us      16.000us             1  \n",
      "                aten::linear         0.06%      16.000us         2.67%     687.000us     687.000us             1  \n",
      "                     aten::t         0.16%      42.000us         0.19%      48.000us      48.000us             1  \n",
      "             aten::transpose         0.02%       4.000us         0.02%       6.000us       6.000us             1  \n",
      "            aten::as_strided         0.01%       2.000us         0.01%       2.000us       2.000us             1  \n",
      "                 aten::addmm         2.32%     597.000us         2.43%     623.000us     623.000us             1  \n",
      "                aten::expand         0.01%       3.000us         0.02%       4.000us       4.000us             1  \n",
      "            aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                 aten::copy_         0.08%      21.000us         0.08%      21.000us      21.000us             1  \n",
      "          aten::resolve_conj         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "          aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "               aten::sigmoid         0.21%      54.000us         0.21%      54.000us      54.000us             1  \n",
      "                aten::linear         0.02%       5.000us         0.32%      83.000us      83.000us             1  \n",
      "                     aten::t         0.02%       6.000us         0.04%      10.000us      10.000us             1  \n",
      "             aten::transpose         0.01%       2.000us         0.02%       4.000us       4.000us             1  \n",
      "            aten::as_strided         0.01%       2.000us         0.01%       2.000us       2.000us             1  \n",
      "                 aten::addmm         0.23%      59.000us         0.26%      68.000us      68.000us             1  \n",
      "                aten::expand         0.00%       1.000us         0.01%       2.000us       2.000us             1  \n",
      "            aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                 aten::copy_         0.03%       7.000us         0.03%       7.000us       7.000us             1  \n",
      "          aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "          aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "               aten::sigmoid         0.21%      54.000us         0.21%      54.000us      54.000us             1  \n",
      "                aten::linear         0.01%       3.000us         0.29%      74.000us      74.000us             1  \n",
      "                     aten::t         0.02%       6.000us         0.04%       9.000us       9.000us             1  \n",
      "             aten::transpose         0.01%       2.000us         0.01%       3.000us       3.000us             1  \n",
      "            aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                 aten::addmm         0.20%      52.000us         0.24%      62.000us      62.000us             1  \n",
      "                aten::expand         0.01%       2.000us         0.01%       2.000us       2.000us             1  \n",
      "            aten::as_strided         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                 aten::copy_         0.03%       7.000us         0.03%       7.000us       7.000us             1  \n",
      "          aten::resolve_conj         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "          aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 25.683ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = torch.rand(256, 1, 28, 28)\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
    "    # for i in range(10):\n",
    "    net(data)\n",
    "print(prof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换为计算图，并打印一些计算图信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_module = fx.symbolic_trace(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode       name     target    args        kwargs\n",
      "-----------  -------  --------  ----------  --------\n",
      "placeholder  input_1  input     ()          {}\n",
      "call_module  _0       0         (input_1,)  {}\n",
      "call_module  _1       1         (_0,)       {}\n",
      "call_module  _2       2         (_1,)       {}\n",
      "call_module  _3       3         (_2,)       {}\n",
      "call_module  _4       4         (_3,)       {}\n",
      "call_module  _5       5         (_4,)       {}\n",
      "call_module  _6       6         (_5,)       {}\n",
      "call_module  _7       7         (_6,)       {}\n",
      "call_module  _8       8         (_7,)       {}\n",
      "call_module  _9       9         (_8,)       {}\n",
      "call_module  _10      10        (_9,)       {}\n",
      "call_module  _11      11        (_10,)      {}\n",
      "output       output   output    (_11,)      {}\n"
     ]
    }
   ],
   "source": [
    "# fx_module 包含一个简单的计算图，可以打印成表格便于查看。我们的目标是将此图转换为 IRModule。\n",
    "fx_module.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了展示torch中nn函数的计算图\n",
    "conv2d = fx.symbolic_trace(net[0])\n",
    "sigmoid = fx.symbolic_trace(net[1])\n",
    "pooling = fx.symbolic_trace(net[2])\n",
    "flatten = fx.symbolic_trace(net[6])\n",
    "linear  = fx.symbolic_trace(net[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name     target                                                     args                                                kwargs\n",
      "-------------  -------  ---------------------------------------------------------  --------------------------------------------------  --------\n",
      "placeholder    input_1  input                                                      ()                                                  {}\n",
      "get_attr       weight   weight                                                     ()                                                  {}\n",
      "get_attr       bias     bias                                                       ()                                                  {}\n",
      "call_function  conv2d   <built-in method conv2d of type object at 0x7fb7924f8780>  (input_1, weight, bias, (1, 1), (2, 2), (1, 1), 1)  {}\n",
      "output         output   output                                                     (conv2d,)                                           {}\n"
     ]
    }
   ],
   "source": [
    "conv2d.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义torch中函数/Module与tir中函数/Module的映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax_wrapper import map_param, from_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def te_avgpool2d(A: te.Tensor, kernel_size, stride) -> te.Tensor:\n",
    "    # print(\"avg pool2d: \", A.shape)\n",
    "    assert(len(A.shape) == 4) # batch_size, channel, height, width\n",
    "    m = A.shape[-2]\n",
    "    n = A.shape[-1]\n",
    "    shape = (A.shape[0], A.shape[1], te.indexdiv(m-kernel_size, stride) + 1, te.indexdiv(n-kernel_size, stride) + 1)\n",
    "    hk = te.reduce_axis((0, kernel_size), name=\"hk\") # row\n",
    "    wk = te.reduce_axis((0, kernel_size), name=\"wk\") # col\n",
    "    return te.compute(shape, lambda b, c, h, w: te.sum(A[b, c, h+hk, w+wk], axis=[hk, wk]), name=\"avgpool2d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义映射Module OP, 这里用的三种方式\n",
    "# 1. 使用TE自定义映射函数\n",
    "# 2. 使用topi中包含的TE函数\n",
    "# 3. 使用relax.op中包含的函数\n",
    "\n",
    "# relax.nn.flatten和relax.nn.conv2d利用现成的pass降级还存在问题, 直接用topi.nn替代了\n",
    "\n",
    "def map_nn_relu_op(bb: relax.BlockBuilder, node_map, node: fx.node.Node, nn_mod: nn.ReLU):\n",
    "    A = node_map[node.args[0]]\n",
    "    return bb.emit(relax.op.relu(A))\n",
    "\n",
    "def map_nn_sigmoid_op(bb: relax.BlockBuilder, node_map, node: fx.node.Node, nn_mod: nn.Sigmoid):\n",
    "    A = node_map[node.args[0]]\n",
    "    return bb.emit_te(topi.sigmoid, A)\n",
    "\n",
    "def map_nn_flatten_op(bb: relax.BlockBuilder, node_map, node: fx.node.Node, nn_mod: nn.Flatten):\n",
    "    A = node_map[node.args[0]]\n",
    "    # return bb.emit(relax.op.flatten(A))\n",
    "    return bb.emit_te(topi.nn.flatten, A)\n",
    "\n",
    "def map_nn_linear_op(bb: relax.BlockBuilder, node_map, node: fx.node.Node, nn_mod: nn.Linear):\n",
    "    x = node_map[node.args[0]]\n",
    "    w = map_param(nn_mod.weight)\n",
    "    if nn_mod.bias is not None:\n",
    "        b = map_param(nn_mod.bias)\n",
    "    y = bb.emit(relax.op.dense(x, w))\n",
    "    return bb.emit(relax.op.add(y, b))\n",
    "\n",
    "def map_nn_conv2d_op(bb: relax.BlockBuilder, node_map, node: fx.node.Node, nn_mod: nn.Conv2d):\n",
    "    # print(\"conv2d: out_channels: \", nn_mod.out_channels)\n",
    "    # print(\"conv2d: kernel_size: \", nn_mod.kernel_size)\n",
    "    # print(\"conv2d: stride: \", nn_mod.stride)\n",
    "    # print(\"conv2d: padding: \", nn_mod.padding)\n",
    "\n",
    "    x = node_map[node.args[0]]\n",
    "    w = map_param(nn_mod.weight)\n",
    "    if nn_mod.bias is not None:\n",
    "        b = map_param(nn_mod.bias)\n",
    "    # return bb.emit(relax.op.conv2d(x, w, channels=nn_mod.out_channels,\n",
    "    #             kernel_size=nn_mod.kernel_size,\n",
    "    #             strides=nn_mod.stride, padding=nn_mod.padding))\n",
    "    return bb.emit_te(topi.nn.conv2d, x, w, nn_mod.stride, nn_mod.padding, nn_mod.dilation)\n",
    "\n",
    "def map_nn_avgpool2d_op(bb: relax.BlockBuilder, node_map, node: fx.node.Node, nn_mod: nn.AvgPool2d):\n",
    "    # print(\"avgpool2d: kernel_size: \", nn_mod.kernel_size)\n",
    "    # print(\"avgpool2d: stride: \", nn_mod.stride)\n",
    "    A = node_map[node.args[0]]\n",
    "    return bb.emit_te(te_avgpool2d, A, nn_mod.kernel_size, nn_mod.stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换为tir，并计算当前基线的运行性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNetModule = from_fx(\n",
    "    fx.symbolic_trace(net),\n",
    "    input_shapes = [(256, 1, 28, 28)],\n",
    "    call_function_map={\n",
    "    },\n",
    "    call_module_map={\n",
    "        torch.nn.Linear: map_nn_linear_op,\n",
    "        torch.nn.Sigmoid: map_nn_sigmoid_op,\n",
    "        torch.nn.ReLU: map_nn_relu_op,\n",
    "        torch.nn.Flatten: map_nn_flatten_op,\n",
    "        torch.nn.Conv2d: map_nn_conv2d_op,\n",
    "        torch.nn.AvgPool2d: map_nn_avgpool2d_op\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.relax.testing import transform\n",
    "from tvm.relax.transform.tuning_api import Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# 降级转为tir\n",
    "target = tvm.target.Target(\"llvm\")\n",
    "with target, tvm.transform.PassContext(trace=Trace(LeNetModule), opt_level=0):\n",
    "    seq = tvm.transform.Sequential(\n",
    "    [\n",
    "        transform.LowerWithRelayOpStrategyPass(target)\n",
    "    ])\n",
    "    NewLeNetModule = seq(LeNetModule)\n",
    "# 合并函数\n",
    "for var, func in LeNetModule.functions.items():\n",
    "    if not isinstance(func, tvm.tir.function.PrimFunc):\n",
    "        continue\n",
    "    if var in NewLeNetModule.functions.keys():\n",
    "        continue\n",
    "    NewLeNetModule[var] = func\n",
    "print(len(NewLeNetModule.functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost of baseline LeNetModule: 0.084377 s\n"
     ]
    }
   ],
   "source": [
    "assert relax.analysis.well_formed(NewLeNetModule)\n",
    "\n",
    "a_np = np.random.rand(256, 1, 28, 28).astype(\"float32\")\n",
    "a_nd = tvm.nd.array(a_np)\n",
    "\n",
    "exec = relax.vm.build(NewLeNetModule, target=target)\n",
    "vm = relax.VirtualMachine(exec, tvm.cpu())\n",
    "\n",
    "f_timer_baseline = vm.time_evaluator(\"main\", tvm.cpu())\n",
    "print(\"Time cost of baseline LeNetModule: %f s\" % (f_timer_baseline(a_nd).mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始优化：基本算子：conv2d：\n",
    "1. 定义优化后的conv2d算子；\n",
    "2. 修改relax函数：改变main中的call_tir指向的var为新的conv2d函数\n",
    "3. 合并函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_opt_function_(func: tvm.tir.PrimFunc, new_symbolname):\n",
    "    # 1. 定义新的func, 并指定其名称\n",
    "    sch = tvm.tir.Schedule(IRModule({new_symbolname: func.with_attr({\"global_symbol\": new_symbolname})}))\n",
    "    # 2. 优化pad_temp block\n",
    "    block_pad = sch.get_block(\"pad_temp\", func_name=new_symbolname)\n",
    "    pad_i, pad_j, pad_m, pad_n = sch.get_loops(block=block_pad)\n",
    "    sch.parallel(pad_i)\n",
    "    sch.unroll(pad_j)\n",
    "    sch.vectorize(pad_n)\n",
    "    # 3. 优化conv2d_nchw block\n",
    "    block_nchw = sch.get_block(\"conv2d_nchw\", func_name=new_symbolname)\n",
    "    # batch/out_channel/pad_h/pad_w/in_channel/kernel_h/kernel_w\n",
    "    i, j, m, n, k_i, k_h, k_w = sch.get_loops(block=block_nchw)\n",
    "    sch.reorder(i, k_i, k_h, k_w, j, m, n)\n",
    "    sch.parallel(i)\n",
    "    sch.unroll(k_i)\n",
    "    sch.vectorize(n)\n",
    "    return sch.mod[new_symbolname]\n",
    "\n",
    "def sigmoid_opt_function_(func: tvm.tir.PrimFunc, new_symbolname):\n",
    "    # 1. 定义新的func, 并指定其名称\n",
    "    sch = tvm.tir.Schedule(IRModule({new_symbolname: func.with_attr({\"global_symbol\": new_symbolname})}))\n",
    "    # 2. 并行化、向量化\n",
    "    block_compute = sch.get_block(\"compute\", func_name=new_symbolname)\n",
    "    loop_axis = sch.get_loops(block=block_compute)\n",
    "    sch.parallel(loop_axis[0])\n",
    "    sch.vectorize(loop_axis[-1]) \n",
    "    return sch.mod[new_symbolname]\n",
    "\n",
    "# [REMOVE]\n",
    "def tir_func_optimizer_schedule(mod: tvm.ir.module.IRModule):\n",
    "    # 1. 移除没有使用的Functions\n",
    "    new_mod = relax.transform.RemoveUnusedFunctions()(mod)\n",
    "    # 2. 针对不同的op进行优化\n",
    "    for global_var, func in new_mod.functions.items():\n",
    "        func_name = func.attrs[\"global_symbol\"]\n",
    "        opt_func_name = func_name + \"_opt\"\n",
    "        if func_name.startswith(\"conv2d\") and not func_name.endswith(\"opt\"):\n",
    "            new_mod[opt_func_name] = conv2d_opt_function_(func, opt_func_name)\n",
    "    return new_mod\n",
    "\n",
    "# 完成对main函数中对tir函数调用的修改\n",
    "@relax.expr_functor.mutator\n",
    "class LeNetModuleRewriter(relax.PyExprMutator):\n",
    "    def __init__(self, mod: IRModule) -> None:\n",
    "        super().__init__()\n",
    "        self.mod_ = mod\n",
    "        self.functions = [call.attrs[\"global_symbol\"] for call in mod.functions.values()]\n",
    "        self.dense_op = tvm.ir.Op.get(\"relax.nn.dense\")\n",
    "        self.add_op = tvm.ir.Op.get(\"relax.add\")\n",
    "\n",
    "    # 处理relax func中的call\n",
    "    def visit_call_(self, call: relax.expr.Call) -> relax.expr.Expr:\n",
    "        call: relax.expr.Call = self.visit_expr_post_order(call)\n",
    "        call_tir_op = tvm.ir.Op.get(\"relax.call_tir\")\n",
    "        # 完成算子替换\n",
    "        if call.op == call_tir_op:\n",
    "            # print(\"[args len]:\", len(call.args), \"[args 0]:\", call.args[0], type(call.args[0]), len(call.args[1]), call.args[2])\n",
    "            # print(self.mod_[call.args[0]])\n",
    "            tir_func: tvm.tir.PrimFunc = self.mod_[call.args[0]]\n",
    "            func_name = tir_func.attrs[\"global_symbol\"]\n",
    "            opt_func_name = func_name + \"_opt\"\n",
    "            if func_name.endswith(\"opt\"): # opt func\n",
    "                return call\n",
    "            if opt_func_name in self.functions: # already opt\n",
    "                return call\n",
    "            # print(opt_func_name, func_name)\n",
    "            if func_name.startswith(\"conv2d\"):\n",
    "                global_var = self.builder_.add_func(conv2d_opt_function_(tir_func, opt_func_name),\n",
    "                        opt_func_name)\n",
    "            elif func_name.startswith(\"sigmoid\"):\n",
    "                global_var = self.builder_.add_func(sigmoid_opt_function_(tir_func, opt_func_name),\n",
    "                        opt_func_name)\n",
    "            else:\n",
    "                global_var = self.builder_.add_func(tir_func, func_name)\n",
    "            return relax.call_tir(func=global_var, args=[x for x in call.args[1]], shape=call.args[2], dtype=\"float32\")\n",
    "        \n",
    "        return call \n",
    "\n",
    "    def transform(self) -> IRModule:\n",
    "        for global_var, func in self.mod_.functions.items():\n",
    "            \n",
    "            if not isinstance(func, relax.Function):\n",
    "                # 复制全部的tir function\n",
    "                # self.builder_.add_func(func, func.attrs[\"global_symbol\"])\n",
    "                continue\n",
    "            # 处理relax function, 找到call指令并优化函数, 和改变call的对象\n",
    "            updated_func = self.visit_expr(func)\n",
    "            updated_func = relax.analysis.remove_all_unused(updated_func)\n",
    "            self.builder_.update_func(global_var, updated_func)\n",
    "        return self.builder_.get()\n",
    "\n",
    "@tvm.ir.transform.module_pass(opt_level=2, name=\"LeNetModuleRewriter\")\n",
    "class LeNetModuleRewriterPass:\n",
    "    \"\"\"The wrapper for the LeNetModuleRewriter pass.\"\"\"\n",
    "    def transform_module(self, mod, ctx):\n",
    "        return LeNetModuleRewriter(mod).transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewLeNetModule_OPT = LeNetModuleRewriterPass()(NewLeNetModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total functions number:  16\n",
      "Time cost of conv2d opt LeNetModule: 0.044542 s\n"
     ]
    }
   ],
   "source": [
    "assert relax.analysis.well_formed(NewLeNetModule_OPT)\n",
    "\n",
    "print(\"total functions number: \", len(NewLeNetModule_OPT.functions))\n",
    "\n",
    "a_np = np.random.rand(256, 1, 28, 28).astype(\"float32\")\n",
    "a_nd = tvm.nd.array(a_np)\n",
    "\n",
    "exec = relax.vm.build(NewLeNetModule_OPT, target=target)\n",
    "vm = relax.VirtualMachine(exec, tvm.cpu())\n",
    "\n",
    "f_timer_baseline = vm.time_evaluator(\"main\", tvm.cpu())\n",
    "print(\"Time cost of conv2d opt LeNetModule: %f s\" % (f_timer_baseline(a_nd).mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始优化：算子融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完成对main函数中对tir函数调用的修改\n",
    "@relax.expr_functor.mutator\n",
    "class LeNetModuleFuseRewriter(relax.PyExprMutator):\n",
    "    def __init__(self, mod: IRModule) -> None:\n",
    "        super().__init__()\n",
    "        self.mod_ = mod\n",
    "        self.functions = [call.attrs[\"global_symbol\"] for call in mod.functions.values()]\n",
    "        self.call_tir_op = tvm.ir.Op.get(\"relax.call_tir\")\n",
    "        self.dense_op = tvm.ir.Op.get(\"relax.nn.dense\")\n",
    "        self.add_op = tvm.ir.Op.get(\"relax.add\")\n",
    "        self.counter = {\"fused_dense_add\": 0}\n",
    "\n",
    "    def visit_call_(self, call: relax.expr.Call) -> relax.expr.Expr:\n",
    "        call: relax.expr.Call = self.visit_expr_post_order(call)\n",
    "        def match_call(node, op):\n",
    "            if not isinstance(node, relax.Call):\n",
    "                return False\n",
    "            return node.op == op\n",
    "        # 完成算子替换\n",
    "        if match_call(call, self.call_tir_op):\n",
    "            # print(\"[args len]:\", len(call.args), \"[args 0]:\", call.args[0], type(call.args[0]), len(call.args[1]), call.args[2])\n",
    "            # print(self.mod_[call.args[0]])\n",
    "            tir_func: tvm.tir.PrimFunc = self.mod_[call.args[0]]\n",
    "            func_name = tir_func.attrs[\"global_symbol\"]\n",
    "            opt_func_name = func_name + \"_opt\"\n",
    "            if func_name.endswith(\"opt\"): # opt func\n",
    "                return call\n",
    "            if opt_func_name in self.functions: # already opt\n",
    "                return call\n",
    "            if func_name.startswith(\"conv2d\"):\n",
    "                global_var = self.builder_.add_func(conv2d_opt_function_(tir_func, opt_func_name),\n",
    "                        opt_func_name)\n",
    "            elif func_name.startswith(\"sigmoid\"):\n",
    "                global_var = self.builder_.add_func(sigmoid_opt_function_(tir_func, opt_func_name),\n",
    "                        opt_func_name)\n",
    "            else:\n",
    "                global_var = self.builder_.add_func(tir_func, func_name)\n",
    "            return relax.call_tir(func=global_var, args=[x for x in call.args[1]], shape=call.args[2], dtype=\"float32\")\n",
    "        # 完成算子融合, relax.op\n",
    "        # pattern match dense => add\n",
    "        if not match_call(call, self.add_op):\n",
    "            return call\n",
    "        # 通过add的第0个参数找到dense\n",
    "        value = self.lookup_binding(call.args[0])\n",
    "        if value is None:\n",
    "            return call\n",
    "        if not match_call(value, self.dense_op):\n",
    "            return call\n",
    "        x = value.args[0]\n",
    "        w = value.args[1]\n",
    "        b = call.args[1]\n",
    "        # 注意，参数绑定的是每个函数, 所需需要为我们的fused创建新的const\n",
    "        # construct a new fused primitive function\n",
    "        param_x = relax.Var(\"x\", x.shape_, x._checked_type_)\n",
    "        param_w = relax.Var(\"w\", w.shape_, w._checked_type_)\n",
    "        param_b = relax.Var(\"b\", b.shape_, b._checked_type_)\n",
    "\n",
    "        bb = relax.BlockBuilder()\n",
    "        fn_name = \"fused_dense_add%d\" % (self.counter[\"fused_dense_add\"])\n",
    "        self.counter[\"fused_dense_add\"] += 1\n",
    "        # [NOTE]注意这里参数绑定的问题\n",
    "        # [NOTE]注意这里emit_func_output需要指定func的output和input\n",
    "        fn_output = None\n",
    "        with bb.function(fn_name, [param_x, param_w, param_b]):\n",
    "            with bb.dataflow():\n",
    "                lv0 = bb.emit(relax.op.dense(param_x, param_w))\n",
    "                lv1 = bb.emit(relax.op.add(lv0, param_b))\n",
    "                assert fn_output is None\n",
    "                fn_output = bb.emit_output(lv1)\n",
    "            bb.emit_func_output(fn_output)\n",
    "\n",
    "        # Add Primitive attribute to the fused funtions\n",
    "        fused_fn = bb.get()[fn_name].with_attr(\"global_symbol\", fn_name)\n",
    "        fused_fn = fused_fn.with_attr(\"Primitive\", 1)\n",
    "        normalized = self.builder_.normalize(fused_fn)\n",
    "        global_var = self.builder_.add_func(normalized, fn_name)\n",
    "        #[NOTE] \n",
    "        return relax.Call(global_var, [x, w, b], None, None)\n",
    "\n",
    "    def transform(self) -> IRModule:\n",
    "        for global_var, func in self.mod_.functions.items():\n",
    "            if not isinstance(func, relax.Function):\n",
    "                continue\n",
    "            # avoid already fused primitive functions\n",
    "            if \"Primitive\" in func.attrs.keys() and func.attrs[\"Primitive\"] != 0:\n",
    "                continue\n",
    "            updated_func = self.visit_expr(func)\n",
    "            updated_func = relax.analysis.remove_all_unused(updated_func)\n",
    "            self.builder_.update_func(global_var, updated_func)\n",
    "\n",
    "        return self.builder_.get()\n",
    "\n",
    "@tvm.ir.transform.module_pass(opt_level=2, name=\"LeNetModuleFuseRewriter\")\n",
    "class LeNetModuleFuseRewriterPass:\n",
    "    \"\"\"The wrapper for the LeNetModuleFuseRewriter pass.\"\"\"\n",
    "    def transform_module(self, mod, ctx):\n",
    "        return LeNetModuleFuseRewriter(mod).transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "FuseLeNetModule = LeNetModuleFuseRewriterPass()(LeNetModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 降级转为tir\n",
    "target = tvm.target.Target(\"llvm\")\n",
    "with target, tvm.transform.PassContext(trace=Trace(LeNetModule), opt_level=0):\n",
    "    seq = tvm.transform.Sequential(\n",
    "    [\n",
    "        transform.LowerWithRelayOpStrategyPass(target)\n",
    "    ])\n",
    "    FuseLeNetModuleTIR = seq(FuseLeNetModule)\n",
    "# 合并函数\n",
    "for var, func in FuseLeNetModule.functions.items():\n",
    "    if not isinstance(func, tvm.tir.function.PrimFunc):\n",
    "        continue\n",
    "    if var in FuseLeNetModuleTIR.functions.keys():\n",
    "        continue\n",
    "    FuseLeNetModuleTIR[var] = func\n",
    "# 【******】将合成的relax函数转化为tir函数\n",
    "FuseLeNetModuleFinal = relax.transform.FuseTIR()(FuseLeNetModuleTIR)\n",
    "assert relax.analysis.well_formed(FuseLeNetModuleFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost of baseline LeNetModule: 0.044521 s\n"
     ]
    }
   ],
   "source": [
    "a_np = np.random.rand(256, 1, 28, 28).astype(\"float32\")\n",
    "a_nd = tvm.nd.array(a_np)\n",
    "\n",
    "exec = relax.vm.build(FuseLeNetModuleFinal, target=\"llvm\")\n",
    "vm = relax.VirtualMachine(exec, tvm.cpu())\n",
    "\n",
    "f_timer_baseline = vm.time_evaluator(\"main\", tvm.cpu())\n",
    "print(\"Time cost of baseline LeNetModule: %f s\" % (f_timer_baseline(a_nd).mean))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
